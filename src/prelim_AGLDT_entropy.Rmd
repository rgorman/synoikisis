---
title: "Dependency direction entropy from AGLDT XML"
output: html_notebook
---


### The data

Several AGLDT files have been provided as input for this exercise. The files are available at zzz.

### The R script
The R code to compute dependency-direction entropy are given below in executable blocks. Brief explanations are also provided.


***
***


### Packages
Several R packages are necessary for this code. These are available at the CRAN sites.  They must be activated for the current session using the **require()** or **library()** function.


```{r}

require(xml2)
require(dplyr)
require(stringr)
require(magrittr)

```

* **xml2** is a somewhat simpler version of the XML package. It is useful for extracting data from XML files.
* **dplyr** is "a grammar of data manipulation." It allows easier handling of tabular data.
* **stringr** is a package supporting character manipulation. It facilitates dealing with text strings.
* **magrittr** provides an easy to use "pipe-like operator" and supporting functions for help in writing readable code.

***
***


### Loading the treebank files

```{r}

input.dir <- "." # sets this object to current directory

files.v <- dir(path=input.dir, pattern=".*xml") # A vector with names of each XML file  from input directory.

doc.xml <- read_xml(file.path(input.dir, files.v[i]))

```

* **input.dir <- "."** This code assumes that the working directory is set to the location of the treebank files. If this is not the case, the code should be adjusted to reflect your own directory structure.

* The **dir()** function will produce a character vector of the names of files in the named directory. The argument **pattern= etc.** will restrict the output to XML files.

* The **files.v** vector can be used to loop through all treebank files in a directory. We will not use this loop in this demo, but will handle the files individually for pedagogical purposes.

* The **read_xml()** function here takes (via the **file.path()** function) a directory and file name and returns an R object, here named **doc.xml**. This object maintains the structure of the XML file. It is a good idea as quickly as possible to move the data from this XML structure (to a list or matrix, etc). XML objects can be difficult to manipulate in R.


***

### Moving from XML to tibble
#### Extracting Sentences

```{r}

sent.xml <- xml_find_all(doc.xml, "//sentence")

```

Because of the structure of the treebank files, dependency-direction entropy must be figured on a sentence by sentence basis. Thus, we extract every sentence element in the doc.xml object and store the output in sent.xml

**xml_find_all()** is a useful function which extracts all XML elements which match the criterion. The function's first argument is the input object. The second argument is an xpath expression identifying the nodes of interest. XPath is a syntax for defining parts of a XML document and navigating through it.



#### Processing word elements
The following process must be applied iteratively to each sentence in sent.xml. This may be done with a loop or a user-defined function. A loop is often slower but is easier for those new to R. We will use a loop here, **but first we will lay out the stages involved without wrapping them in a loop. A looped version will be included later in this file.** 



##### Extracting word attributes


```{r}

word.l <- sent.xml[[1]] %>%
  xml_find_all("word") %>%
  xml_attrs()

```

This code extracts the attributes of each word node in the first sentence of sent.xml. The output is stored in the list word.l. 

* **sent.xml[[1]] %>%  xml_find_all("word")** sends the data in the first sentence element in sent.xml to **xml_find_all()**. This function extracts all word nodes in that sentence.
* **%>% xml_attrs()** sends the word nodes to **xml_attrs()**, which extracts all attributes in the words and outputs a named character vector for the attributes of each word. The resulting word.l is a list object with a number of elements equal to the number of words in the input sentence.


##### From list to tibble 


```{r}

word.l <- word.l[which(lengths(word.l) == 7)]

sent.workng.m <- unlist(word.l)  %>%
  matrix(ncol = 7, byrow = TRUE)

colnames(sent.workng.m) <- c("id", "form", "lemma", "postag", "relation", "head", "cite")

sent.tib <-  as_tibble(sent.workng.m)

sent.tib$id <- sent.tib$id %>%
  as.numeric()

sent.tib$head <- sent.tib$head %>%
  as.numeric()


```

The annotation scheme used in the AGLDT allows the insertion of ellipses when required by grammatical structure. Because these inserted "words" do not have an actual position in the linear order of the sentence, they should not be considered when extracting data about word order. 

* **word.l <- word.l[which(lengths(word.l) == 7)]** eliminates ellipsis nodes. Normal (non-ellipsis) nodes have 7 attributes. Ellipsis nodes have additional attributes. A simple way to drop the unwanted information is to apply the accessor function ([...]) to **word.l**. 
* Within the square brackets is **lengths(word.l)**, which returns a vector with the length of the vector in each list element in **word.l**. 
* Wrapped around **lengths()** is **which(x == 7)**. This function produces a vector giving the list element index of all those elements which contain vectors of length 7. 
* This nesting of functions may produce a shorter version of **word.l** from which all ellipsis nodes have been dropped.

* **unlist(word.l)  %>%** produces a character vector of a length equal to 7 times the number of words in **word.l** and passes it to the next line of code. This step is preliminary to creating a matrix and then a data frame (a tibble here).
* **matrix(ncol = 7, byrow = TRUE)** produces a matrix object with 7 columns and as many rows as words in the input **word.l**.


* **colnames(sent.workng.m) <- c("id", "form", "lemma", "postag", "relation", "head", "cite")** sets a name for each column in the matrix. Tibbles resist being created from a matrix without column names.

* **sent.tib <-  as_tibble(sent.workng.m)** creates a tibble from the matrix. Tibbles are easier to work with and manipulate.


* In the remaining code in this block, the **as.numeric()** function changes the content of the columns indicated from character strings to numbers. **read_xml()** seems to output the values of all elements and attributes as text. 


### Computing Dependency Direction



```{r}

sent.tib <- sent.tib %>%
  mutate(arc_direction = NA)

sent.tib <- sent.tib[- which(str_detect(sent.tib$relation, "Aux[xkgXKG]") == TRUE) , ]

for (j in seq_len(nrow(sent.tib))) {
  
  if (is.numeric(sent.tib$head[j]) ) {
    
    if (sent.tib$head[j] %>%
        is_in( sent.tib$id)) {
      
      
      if (sent.tib$id[j] < sent.tib$head[j]) {sent.tib$arc_direction[j] <- "parent_follows"}
      if (sent.tib$id[j] > sent.tib$head[j]) {sent.tib$arc_direction[j] <- "parent_precedes"}
      
      
    }
  }
  
}


```




* **sent.tib <- sent.tib %>% mutate(arc_direction = NA)** adds a column called "arc_direction" to **sent.tib** and populates that column with the logical value *NA*. Code calculating the values for each cell in the new column could be included as an argument to **mutate()**. Here, the relevant code is separated from **mutate()** to make it easier to read.
* **sent.tib <- sent.tib[- which(str_detect(sent.tib_dollar_relation, "Aux[xkgXKG]") == TRUE) , ]** removes rows representing unused punctuation tokens from **sent.tib**. 
  * **str_detect(string, pattern)** matches the specified pattern in the input string. Here the input string is the value of the **relation** column for each row of **sent.tib**. The pattern is a RegEx expression to identify punctuation marks which will not have dependencies: AuxX, AuxK, and AuxG. The AGLDT annotation scheme allows punctuation to have dependencies, but in this case the punctuation will be annotated with either the COORD or APOS relationship label.
  * **which(str_detect(sent.tib_dollar_relation, "Aux[xkgXKG]") == TRUE)** returns the index number of the appropriate rows.
  * **<- sent.tib[- ... , ]**: only data from the specified rows are assigned to **sent.tib**, effectively dropping unwanted data. Note that the minus sign preceding the **which()** function selects the rows for which the following criterion is NOT applicable.
  
* The **for()** loop iterates through the rows of **sent.tib** and inserts values in the "arc_direction" column. **seq_len(nrow(sent.tib))** returns an integer vector beginning with 1 and ending with the number of rows in **sent.tib**. This output allows row-by-row manipulation of the input tibble. 
* The **if (is.numeric(sent.tib_dollar_head[j]) )** condition prevents the code from breaking when there is an annotation error and some token in a sentence has not been assigned a *head* value. If the criterion is not met, the rest of the loop is bypassed. 
* The **if (sent.tib_dollar_head[j] %>% is_in(sent.tib_dollar_id))** condition drops from the loop tokens which are dependent on a supplied ellipsis token. All ellipsis nodes themselves have been dropped from the data at a previous step of the script. The dependents of these dropped nodes still remain in the data, but should not be assigned a dependency direction. On the other hand, dependents of ellipses should not be dropped, since they do appear in the linear order of words in a sentence and, crucially, may have dependents of their own for which a dependency direction should be calculated. **sent.tib_dollar_id** generates a vector containing the "id" values for all rows, and then **is_in()** checks to see that the value of the "head" column for the target row **[j]** is an element of that vector. If the condition is TRUE, then the target word is not dependent on an ellipsis (because they have been dropped and have no corresponding "id" value), and the the token is processed. Otherwise, the rest of the loop is bypassed. 
* The **if (sent.tib_dollar_id[j] < sent.tib_dollar_head[j]) {sent.tib_dollar_arc_direction[j] <- "parent_follows"}** condition compares the value of the target's "id" column to the value of its "head" column. An "id" value smaller than a "head" value means that the dependency parent follows the target token, so the corresponding value is assigned to the "arc_direction" column. The complementary condition is given in the next line.


#### Rewriting part-of-speech for ease of reading


```{r}

sent.tib <- sent.tib %>%
  mutate(part_of_speech = NA)

for (k in seq_len(nrow(sent.tib))) {
  
  if (str_detect(sent.tib$postag[k], "^-")) {
    
  } else {
    postag <-  str_extract(sent.tib$postag[k], "^[a-z]")
    
    if (postag == "v") {
      sent.tib$part_of_speech[k] <- "verb"
    }
    if (postag == "n") {
      sent.tib$part_of_speech[k] <- "noun"
    }
    if (postag == "p") {
      sent.tib$part_of_speech[k] <- "pronoun"
    }
    if (postag == "c") {
      sent.tib$part_of_speech[k] <- "conjunction"
    }
    if (postag == "d") {
      sent.tib$part_of_speech[k] <- "adverb"
    } 
    if (postag == "l") {
      sent.tib$part_of_speech[k] <- "def_article"
    }
    if (postag == "r") {
      sent.tib$part_of_speech[k] <- "preposition"
    }
    if (postag == "a") {
      sent.tib$part_of_speech[k] <- "adjective"
    }
    if (postag == "u") {
      sent.tib$part_of_speech[k] <- "punctuation"
    }
    if (postag == "NA") {
      sent.tib$part_of_speech[k] <- NA
    }
    
  }
  
}


```

* **sent.tib <- sent.tib %>% mutate(part_of_speech = NA)** creates a new column in **sent.tib** and populates it with the logical value NA
* the condition **str_detect(sent.tib$postag[k], "^-")** checks the first character in the string in each "postag" column cell. Using the RegEx expression "^-", the condition is TRUE if the first character is a dash, indicating that no part of speech has been assigned to this token. If the condition is TRUE, the rest of the loop is skipped.



#### Adding data for dependency parents

```{r}

sent.tib <- sent.tib %>%
  mutate(parent_POS = NA)

sent.tib <- sent.tib %>%
  mutate(parent_relation = NA)



y <- sent.tib$id 

for (p in seq_len(nrow(sent.tib))) {
  
  x <- sent.tib$head[p] 
  
  if (x %in% y) {
    if (x > 0) {
      sent.tib$parent_POS[p] <- sent.tib$part_of_speech[which(sent.tib$id == x) ]
      sent.tib$parent_relation[p] <- sent.tib$relation[which(sent.tib$id == x) ]
    }
    
  }
}



```


Dependency-direction entropy is more useful when it can be conditioned on syntactic features of the context.  The part-of-speech and relation annotation of the parent word are among the most important of such features. This code block extracts this information and adds it to **sent.tib**.

* New columns are added to the tibble and populated with NAs with the **mutate()** function, as in previous steps.
* **y <- sent.tib$id** creates a numeric vector with all id values from **sent.tib**. Since the contents of this vector must be checked for each row in **sent.tib**, it is placed before the **for()** loop, to reduce redundancy.
* This **for()** loop iterates through all rows in **sent.tib**. 
* **x <- sent.tib$head[p]** populates a vector with an integer from the "head" column of the current row. This value identifies the parent of the current token. 
* The first of the nested conditions, **if (x %in% y) { ... }** checks to see if the current token is the dependency of an ellipsis node. No syntactic information should be extracted for such tokens. If the "head" value of the current token is not in the vector of all "id" values in **sent.tib**, the condition evaluates as FALSE and the remainder of the loop is skipped.
* The second of the nested conditions checks to see that the current token is not the sentence root. Such tokens are given a parent "id" value of 0. There is no actual token zero in a sentence; syntactic data are inapplicable here. If the condition is FALSE, the current token is a root and the rest of the loop is skipped.
* **sent.tib$parent_POS[p] <- sent.tib$part_of_speech[which(sent.tib$id == x) ]** populates the "parent_POS" column with the appropriate value. The function **which(sent.tib$id == x)** is used to identify the row containing the parent of the current token: the "head" value of the token is in x; x must match the "id" value of some row in **sent.tib**. The "part_of_speech" value of the matched row is extracted using sub-scripting (via the **[...]** function applied to **sent.tib$part_of_speech**).
* **sent.tib$parent_relation[p] <- sent.tib$relation[which(sent.tib$id == x) ]** uses the same method (mutatis mutandis) to populate the "parent_relation" column.


###A loop to handle all sentences in a file

The following loop strings together all the processes applied above to **sent.tib**.


```{r}


for (i in seq_along(sent.xml)) {
  
  word.l <- sent.xml[[i]] %>%
  xml_find_all("word") %>%
  xml_attrs()
  
  word.l <- word.l[which(lengths(word.l) == 7)]

sent.workng.m <- unlist(word.l)  %>%
  matrix(ncol = 7, byrow = TRUE)

colnames(sent.workng.m) <- c("id", "form", "lemma", "postag", "relation", "head", "cite")

sent.tib <-  as_tibble(sent.workng.m)

sent.tib$id <- sent.tib$id %>%
  as.numeric()

sent.tib$head <- sent.tib$head %>%
  as.numeric()

  
sent.tib <- sent.tib %>%
  mutate(arc_direction = NA)

sent.tib <- sent.tib[- which(str_detect(sent.tib$relation, "Aux[xkgXKG]") == TRUE) , ]

for (j in seq_len(nrow(sent.tib))) {
  
  if (is.numeric(sent.tib$head[j]) ) {
    
    if (sent.tib$head[j] %>%
        is_in( sent.tib$id)) {
      
      
      if (sent.tib$id[j] < sent.tib$head[j]) {sent.tib$arc_direction[j] <- "parent_follows"}
      if (sent.tib$id[j] > sent.tib$head[j]) {sent.tib$arc_direction[j] <- "parent_precedes"}
      
      
    }
  }
  
}

sent.tib <- sent.tib %>%
  mutate(part_of_speech = NA)

for (k in seq_len(nrow(sent.tib))) {
  
  if (str_detect(sent.tib$postag[k], "^-")) {
    
  } else {
    postag <-  str_extract(sent.tib$postag[k], "^[a-z]")
    
    if (postag == "v") {
      sent.tib$part_of_speech[k] <- "verb"
    }
    if (postag == "n") {
      sent.tib$part_of_speech[k] <- "noun"
    }
    if (postag == "p") {
      sent.tib$part_of_speech[k] <- "pronoun"
    }
    if (postag == "c") {
      sent.tib$part_of_speech[k] <- "conjunction"
    }
    if (postag == "d") {
      sent.tib$part_of_speech[k] <- "adverb"
    } 
    if (postag == "l") {
      sent.tib$part_of_speech[k] <- "def_article"
    }
    if (postag == "r") {
      sent.tib$part_of_speech[k] <- "preposition"
    }
    if (postag == "a") {
      sent.tib$part_of_speech[k] <- "adjective"
    }
    if (postag == "u") {
      sent.tib$part_of_speech[k] <- "punctuation"
    }
    if (postag == "NA") {
      sent.tib$part_of_speech[k] <- NA
    }
    
  }
  
}


sent.tib <- sent.tib %>%
  mutate(parent_POS = NA)

sent.tib <- sent.tib %>%
  mutate(parent_relation = NA)



y <- sent.tib$id 

for (p in seq_len(nrow(sent.tib))) {
  
  x <- sent.tib$head[p] 
  
  if (x %in% y) {
    if (x > 0) {
      sent.tib$parent_POS[p] <- sent.tib$part_of_speech[which(sent.tib$id == x) ]
      sent.tib$parent_relation[p] <- sent.tib$relation[which(sent.tib$id == x) ]
    }
    
  }
}


 if ( i == 1) {
    result.tib <- sent.tib
  } else {
    result.tib <- bind_rows(result.tib, sent.tib)
  }


  
}



```

* The only new code in this loop is the final conditional which collects the results from each sentence iteration into a single tibble:
* **if ( i == 1) {result.tib <- sent.tib}** creates **result.tib** on the first iteration through **sent.xml** and populates it with the contents of **sent.tib**.
* **else {result.tib <- bind_rows(result.tib, sent.tib)}** uses the **bind_rows()** function to add the results of further iterations through **sent.xml** as rows to the end of of **result.tib**.



##Calculating Dependency-Direction Entropy Under Specified Conditions


###Manipulating tibbles by condition of interest



Entropy is most interesting in discussions of word order when entropy of that order is conditioned on some other feature. Here those figures are syntactic. For example, we may ask how much information, on average, is associated with the order of a definite article and its parent.  Answer: not very much. What about the word order of a dependent verb and its parent. Answer: practically the maximal amount of information.

To ask these kind of questions, it is useful to manipulate our tibble by part-of-speech, relation, etc. The following block of code shows a way of doing that.


```{r}

entr.tib <- result.tib %>%
  group_by(part_of_speech, arc_direction) %>%
  na.omit() %>%
  summarize(count =  n()) 



```

* The **group_by()** function is an easy way to condition variables. This function creates a grouped data frame.  Various calculations can then be perfomred on the groups. 
* Here **group_by(part_of_speech, arc_direction)** takes the imput tibble and generates a new tibble with 18 rows. There are nine parts-of-speech, so the result of that grouping is a nine-row tibble. There are two possible dependency-directions, for every word. Thus, associating every part-of-speech with the relevant data for word order produces 18 rows total.




###Calculating Entropy


```{r}


z <- unique(entr.tib[, 1]) %>%
  unlist() %>%
  as.character()




for (j in seq_along(z)) {
  
  row.traker.v  <- which(entr.tib[, 1] == z[j])
 
  probability.v <- entr.tib$count[row.traker.v ] %>%
    divide_by(entr.tib$count[row.traker.v ] %>%
                sum() )
  
  entr.tib$probs[row.traker.v] <- probability.v
  
  surprisal.v <- probability.v %>%
    log2() %>%
    multiply_by(-1)
  
  entr.tib$surprisal[row.traker.v] <- surprisal.v
  
  H.holder.v <- surprisal.v %>%
    multiply_by(probability.v)
  
  entr.tib$entropy[row.traker.v] <- H.holder.v
  
  H.summed.v <- H.holder.v %>%
    sum()
  
  entr.tib$summed_entropy[row.traker.v] <- H.summed.v
  
 
  
} 


```





###############

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
